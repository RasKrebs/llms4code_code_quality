# Application domain info yaml, to store data used during data collection

# General Application Domain Info
domain: Machine Learning
application: Language Modelling
algorithm: Multi-Head Attention

# Prompts
long_prompt: """
Implement a MultiHeadAttention Python class for transformer models completely from scratch,
which means torch or any other popular deep learning frameworks can't be used. 
It should initialize with emb_size (input embedding size) and num_heads (number of 
attention heads), validating that emb_size is divisible by num_heads. Initialize weights 
for query, key, value, and output projections with random values. Include a 
scaled_dot_product_attention method to perform the attention operation, scaling the dot 
product of query and key by the query depth's square root, applying softmax for stability, 
and multiplying by the value tensor. The forward method projects input tensors into query, 
key, and value tensors using the initialized weights, applies attention, concatenates the 
results, and applies an output projection Implement the necessary steps to handle edge 
cases and common errors. 

The implementation should be well-documented with comments and docstrings, and should 
follow common conventions for Python code. Type hints should be used throughout.

"""
medium_prompt: """
Implement a MultiHeadAttention from scratch in python, which performs the multi-head 
attention mechanism. Torch or any other popular deep learning frameworks can't be used.
At initializion, emb_size and num_heads are passed, make sure to calculate the dimension 
of each head ensuring divisibility. Generate random weight matrices for query, key, value, 
and output projections. Include a method for scaled dot-product attention that can handle 
numeric overflow. Also apply softmax. A forward method should perform the multi-head 
attention and return the output. 

The implementation should be well-documented with comments and docstrings, and should 
follow common conventions for Python code. Type hints should be used throughout.
"""

small_prompt: """
Produce an implementation fo MultiHeadAttention in Python from scratch. Torch or any 
other popular deep learning frameworks can't be used.It should take embedding size 
and number of heads at initialization, and produce random weight and a forward method 
should take the input tensor and apply multi-head attention.
"""